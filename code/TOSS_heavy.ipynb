{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21991,"status":"ok","timestamp":1759996535906,"user":{"displayName":"김성윤","userId":"09187544673822150877"},"user_tz":-540},"id":"LntquHVd1wN_","outputId":"8716c040-0846-4a01-8ffe-df34ac695aff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n","/gdrive/MyDrive/open\n"]}],"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","\n","%cd /gdrive/MyDrive/open/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Sl5V-xzECDK"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","import torch.nn.utils.rnn as rnn_utils\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import average_precision_score\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","\n","all_train = pd.read_parquet(\"./train.parquet\", engine=\"pyarrow\")\n","test = pd.read_parquet(\"./test.parquet\", engine=\"pyarrow\").drop(columns=['ID'])\n","train_df = all_train.sample(frac=1, random_state=42).reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bSJqWXrKmX_o"},"outputs":[],"source":["target_col = \"clicked\"\n","seq_col = \"seq\"\n","\n","categorical_cols = ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour', 'l_feat_14']\n","numerical_feat_cols = [c for c in train_df.columns if c.startswith('feat_')]\n","numerical_history_cols = [c for c in train_df.columns if c.startswith('history_')]\n","numerical_l_feat_cols = [c for c in train_df.columns if c.startswith('l_feat_') and c not in categorical_cols]\n","numerical_cols = numerical_feat_cols + numerical_history_cols + numerical_l_feat_cols\n","\n","# Label Encoding\n","vocab_sizes = {}\n","for col in categorical_cols:\n","    all_categories = pd.concat([all_train[col], test[col]]).astype(str).unique()\n","    vocab_sizes[col] = len(all_categories) + 1\n","    le = LabelEncoder()\n","    le.fit(all_categories)\n","    train_df[col] = le.transform(train_df[col].astype(str)).astype(int)\n","    test[col] = le.transform(test[col].astype(str)).astype(int)\n","\n","# StandardScaler\n","scaler = StandardScaler()\n","train_df[numerical_cols] = scaler.fit_transform(train_df[numerical_cols])\n","test[numerical_cols] = scaler.transform(test[numerical_cols])\n","\n","# NaN / Inf 처리\n","train_df[numerical_cols] = train_df[numerical_cols].fillna(0).replace([np.inf, -np.inf], 0)\n","test[numerical_cols] = test[numerical_cols].fillna(0).replace([np.inf, -np.inf], 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"293ad917"},"outputs":[],"source":["class ClickDataset(Dataset):\n","    def __init__(self, df, categorical_cols, numerical_feat_cols, numerical_history_cols, numerical_l_feat_cols, seq_col, target_col=None, has_target=True):\n","        self.df = df.reset_index(drop=True)\n","        self.categorical_cols = categorical_cols\n","        self.numerical_feat_cols = numerical_feat_cols\n","        self.numerical_history_cols = numerical_history_cols\n","        self.numerical_l_feat_cols = numerical_l_feat_cols\n","        self.seq_col = seq_col\n","        self.target_col = target_col\n","        self.has_target = has_target\n","        if self.has_target:\n","            self.y = self.df[self.target_col].astype(np.float32).values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        cat_features = torch.tensor(self.df.loc[idx, self.categorical_cols].values.astype(int), dtype=torch.long)\n","        num_feat = torch.tensor(self.df.loc[idx, self.numerical_feat_cols].values.astype(float), dtype=torch.float)\n","        num_history = torch.tensor(self.df.loc[idx, self.numerical_history_cols].values.astype(float), dtype=torch.float)\n","        num_l_feat = torch.tensor(self.df.loc[idx, self.numerical_l_feat_cols].values.astype(float), dtype=torch.float)\n","\n","        seq_string = self.df.loc[idx, self.seq_col]\n","        seq_array = np.fromstring(seq_string, sep=\",\", dtype=np.float32) if isinstance(seq_string, str) and seq_string else np.array([0.0], dtype=np.float32)\n","        seq_tensor = torch.from_numpy(seq_array)\n","\n","        if self.has_target:\n","            y = torch.tensor(self.y[idx], dtype=torch.float)\n","            return cat_features, num_feat, num_history, num_l_feat, seq_tensor, y\n","        else:\n","            return cat_features, num_feat, num_history, num_l_feat, seq_tensor\n","\n","def collate_fn_train(batch):\n","    cat_features, num_feat, num_history, num_l_feat, seqs, ys = zip(*batch)\n","    cat_features = torch.stack(cat_features)\n","    num_feat = torch.stack(num_feat)\n","    num_history = torch.stack(num_history)\n","    num_l_feat = torch.stack(num_l_feat)\n","    ys = torch.stack(ys).unsqueeze(1)\n","\n","    seqs_padded = rnn_utils.pad_sequence(seqs, batch_first=True, padding_value=0.0)\n","    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n","    seq_lengths = torch.clamp(seq_lengths, min=1)\n","    return cat_features, num_feat, num_history, num_l_feat, seqs_padded, seq_lengths, ys\n","\n","def collate_fn_infer(batch):\n","    cat_features, num_feat, num_history, num_l_feat, seqs = zip(*batch)\n","    cat_features = torch.stack(cat_features)\n","    num_feat = torch.stack(num_feat)\n","    num_history = torch.stack(num_history)\n","    num_l_feat = torch.stack(num_l_feat)\n","    seqs_padded = rnn_utils.pad_sequence(seqs, batch_first=True, padding_value=0.0)\n","    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n","    seq_lengths = torch.clamp(seq_lengths, min=1)\n","    return cat_features, num_feat, num_history, num_l_feat, seqs_padded, seq_lengths"]},{"cell_type":"markdown","metadata":{"id":"b8996f2c"},"source":["## 모델 구조 정의\n","\n","### Subtask:\n","사용자 요구사항에 맞춰 새로운 `ComplexModel` 클래스를 정의합니다. 이 클래스 안에는 다음 서브 모듈이 포함됩니다.\n","- 범주형 피처를 위한 임베딩 레이어와 Transformer Encoder.\n","- 시퀀스 데이터 처리를 위한 LSTM.\n","- `feat_*_*` 수치형 피처를 위한 MLP.\n","- `history_*_*` 수치형 피처를 위한 MLP.\n","- 모든 서브 모듈의 출력을 결합하고 최종 예측을 수행하는 MLP."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d00a2e59"},"outputs":[],"source":["class ComplexModel(nn.Module):\n","    def __init__(self, vocab_sizes, embedding_dims, numerical_feat_dim, numerical_history_dim, numerical_l_feat_dim,\n","                 lstm_hidden_dim=64, lstm_layers=1,\n","                 mlp_hidden_units_feat=[160, 80], mlp_hidden_units_history=[64, 32], mlp_hidden_units_l_feat=[32, 16],\n","                 final_mlp_hidden_units=[512, 256, 128], dropout=0.2):\n","        super().__init__()\n","\n","        # Embeddings for categorical features\n","        self.embedding_layers = nn.ModuleDict({col: nn.Embedding(vocab_sizes[col], embedding_dims[col]) for col in vocab_sizes})\n","        total_cat_dim = sum(embedding_dims.values())\n","\n","        # LSTM for sequence data\n","        self.lstm = nn.LSTM(input_size=1, hidden_size=lstm_hidden_dim, num_layers=lstm_layers, batch_first=True)\n","\n","\n","        def create_mlp(input_dim, hidden_units):\n","            layers = []\n","            for h in hidden_units:\n","                layers += [nn.Linear(input_dim, h), nn.ReLU(), nn.Dropout(dropout)]\n","                input_dim = h\n","            return nn.Sequential(*layers), input_dim\n","\n","        self.feat_mlp, feat_out_dim = create_mlp(numerical_feat_dim, mlp_hidden_units_feat)\n","        self.history_mlp, history_out_dim = create_mlp(numerical_history_dim, mlp_hidden_units_history)\n","        self.l_feat_mlp, l_feat_out_dim = create_mlp(numerical_l_feat_dim, mlp_hidden_units_l_feat)\n","\n","\n","        # Final MLP\n","        final_input_dim = total_cat_dim + lstm_hidden_dim + feat_out_dim + history_out_dim + l_feat_out_dim # Adjusted input dim\n","        final_layers = []\n","        for h in final_mlp_hidden_units:\n","            final_layers += [nn.Linear(final_input_dim, h), nn.ReLU(), nn.Dropout(dropout)]\n","            final_input_dim = h\n","        final_layers += [nn.Linear(final_input_dim, 1)]\n","        self.final_mlp = nn.Sequential(*final_layers)\n","\n","    def forward(self, cat_x, num_feat_x, num_history_x, num_l_feat_x, seq_x, seq_lengths):\n","        # Embeddings for categorical features\n","        cat_embs = [self.embedding_layers[col](cat_x[:, i]) for i, col in enumerate(self.embedding_layers)]\n","        cat_embedded = torch.cat(cat_embs, dim=1)\n","\n","        # LSTM processing for sequence\n","        seq_x = seq_x.unsqueeze(-1) # Add feature dimension for LSTM\n","        packed_seq = rnn_utils.pack_padded_sequence(seq_x, seq_lengths.cpu(), batch_first=True, enforce_sorted=False)\n","        lstm_out, _ = self.lstm(packed_seq)\n","        lstm_out, _ = rnn_utils.pad_packed_sequence(lstm_out, batch_first=True)\n","\n","        # Get the output of the last time step\n","        idx = (seq_lengths - 1).view(-1, 1).unsqueeze(1).expand(-1, -1, lstm_out.size(2))\n","        lstm_last_output = torch.gather(lstm_out, 1, idx).squeeze(1)\n","        # lstm_processed = self.lstm_output_proj(lstm_last_output) # Removed\n","        lstm_processed = lstm_last_output # Use raw LSTM output\n","\n","        # Numerical features\n","        feat_out = self.feat_mlp(num_feat_x)\n","        history_out = self.history_mlp(num_history_x)\n","        l_feat_out = self.l_feat_mlp(num_l_feat_x)\n","\n","        # Combine all features\n","        combined = torch.cat([cat_embedded, lstm_processed, feat_out, history_out, l_feat_out], dim=1)\n","        logits = self.final_mlp(combined).squeeze(1)\n","        return logits\n","\n","def calculate_weighted_logloss(y_true, y_pred, eps=1e-15):\n","    y_pred = np.clip(y_pred, eps, 1 - eps)\n","    mask_0 = (y_true == 0)\n","    mask_1 = (y_true == 1)\n","    ll_0 = -np.mean(np.log(1 - y_pred[mask_0])) if mask_0.sum() > 0 else 0\n","    ll_1 = -np.mean(np.log(y_pred[mask_1])) if mask_1.sum() > 0 else 0\n","    return 0.5 * ll_0 + 0.5 * ll_1\n","\n","def calculate_competition_score(y_true, y_pred):\n","    ap = average_precision_score(y_true, y_pred)\n","    wll = calculate_weighted_logloss(y_true, y_pred)\n","    score = 0.5 * ap + 0.5 * (1 / (1 + wll))\n","    return score, ap, wll"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a23ec418"},"outputs":[],"source":["def train_model(train_df, categorical_cols, numerical_feat_cols, numerical_history_cols, numerical_l_feat_cols, seq_col, target_col,\n","                vocab_sizes, embedding_dims, batch_size=1024, epochs=5, lr=1e-3, device=\"cuda\"):\n","\n","    tr_df, va_df = train_test_split(train_df, test_size=0.2, random_state=42, shuffle=True)\n","    train_dataset = ClickDataset(tr_df, categorical_cols, numerical_feat_cols, numerical_history_cols, numerical_l_feat_cols, seq_col, target_col)\n","    val_dataset   = ClickDataset(va_df, categorical_cols, numerical_feat_cols, numerical_history_cols, numerical_l_feat_cols, seq_col, target_col)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_train, num_workers=2)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_train, num_workers=2)\n","\n","    model = ComplexModel(vocab_sizes, embedding_dims,\n","                                    len(numerical_feat_cols), len(numerical_history_cols), len(numerical_l_feat_cols)).to(device)\n","\n","    pos_count = tr_df[target_col].sum()\n","    neg_count = len(tr_df) - pos_count\n","    pos_weight = torch.tensor(neg_count / pos_count, dtype=torch.float32).to(device)\n","    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n","\n","    best_val_score = -float('inf')\n","    history = {'train_losses': [], 'val_losses': [], 'train_comp': [], 'val_comp': []}\n","\n","    for epoch in range(1, epochs+1):\n","        model.train()\n","        train_loss = 0.0\n","        train_preds, train_true = [], []\n","        for cat_x, num_feat, num_history, num_l_feat, seq_x, seq_len, y in tqdm(train_loader):\n","            cat_x, num_feat, num_history, num_l_feat, seq_x, seq_len, y = cat_x.to(device), num_feat.to(device), num_history.to(device), num_l_feat.to(device), seq_x.to(device), seq_len.to(device), y.to(device)\n","            optimizer.zero_grad()\n","            logits = model(cat_x, num_feat, num_history, num_l_feat, seq_x, seq_len)\n","            loss = criterion(logits, y.squeeze(1))\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item() * y.size(0)\n","            train_preds.append(torch.sigmoid(logits).detach().cpu().numpy())\n","            train_true.append(y.squeeze(1).cpu().numpy())\n","\n","        train_loss /= len(train_dataset)\n","        train_preds = np.concatenate(train_preds)\n","        train_true = np.concatenate(train_true)\n","        train_score, _, _ = calculate_competition_score(train_true, train_preds)\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0.0\n","        val_preds, val_true = [], []\n","        with torch.no_grad():\n","            for cat_x, num_feat, num_history, num_l_feat, seq_x, seq_len, y in val_loader:\n","                cat_x, num_feat, num_history, num_l_feat, seq_x, seq_len, y = cat_x.to(device), num_feat.to(device), num_history.to(device), num_l_feat.to(device), seq_x.to(device), seq_len.to(device), y.to(device)\n","                logits = model(cat_x, num_feat, num_history, num_l_feat, seq_x, seq_len)\n","                loss = criterion(logits, y.squeeze(1))\n","                val_loss += loss.item() * y.size(0)\n","                val_preds.append(torch.sigmoid(logits).cpu().numpy())\n","                val_true.append(y.squeeze(1).cpu().numpy())\n","\n","        val_loss /= len(val_dataset)\n","        val_preds = np.concatenate(val_preds)\n","        val_true = np.concatenate(val_true)\n","        val_score, _, _ = calculate_competition_score(val_true, val_preds)\n","\n","        history['train_losses'].append(train_loss)\n","        history['val_losses'].append(val_loss)\n","        history['train_comp'].append(train_score)\n","        history['val_comp'].append(val_score)\n","\n","        if val_score > best_val_score:\n","            best_val_score = val_score\n","            torch.save(model.state_dict(), './best_model.pth')\n","\n","        print(f\"[Epoch {epoch}] Train Loss:{train_loss:.4f}, Val Loss:{val_loss:.4f}, Train Score:{train_score:.4f}, Val Score:{val_score:.4f}\")\n","\n","    model.load_state_dict(torch.load('./best_model.pth'))\n","    return model, history"]},{"cell_type":"markdown","metadata":{"id":"db01c42e"},"source":["## 모델 학습, 추론 및 제출 파일 생성 (컬럼 목록 정의 포함)\n","\n","### Subtask:\n","정의된 `ComplexModel`을 사용하여 모델을 학습하고, 학습된 모델로 테스트 데이터에 대한 추론을 수행하여 제출 파일을 생성합니다. 필요한 컬럼 목록 변수들을 이 셀 내에서 명확히 정의합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"elapsed":127722,"status":"error","timestamp":1759997044508,"user":{"displayName":"김성윤","userId":"09187544673822150877"},"user_tz":-540},"id":"tsCGTsXy7vKF","outputId":"35f2a789-5a82-4e22-9df1-a16e78db7f59"},"outputs":[{"output_type":"stream","name":"stderr","text":["  1%|          | 74/8363 [01:24<2:37:32,  1.14s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2772639513.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0membedding_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'gender'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'age_group'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inventory_id'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'day_of_week'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hour'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'l_feat_14'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m model, history = train_model(train_df, categorical_cols, numerical_feat_cols, numerical_history_cols, numerical_l_feat_cols, seq_col, target_col,\n\u001b[0m\u001b[1;32m      4\u001b[0m                              vocab_sizes, embedding_dims, device=device)\n","\u001b[0;32m/tmp/ipython-input-1172775000.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_df, categorical_cols, numerical_feat_cols, numerical_history_cols, numerical_l_feat_cols, seq_col, target_col, vocab_sizes, embedding_dims, batch_size, epochs, lr, device)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_l_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["embedding_dims = {'gender':4, 'age_group':4, 'inventory_id':4, 'day_of_week':4, 'hour':4, 'l_feat_14':16}\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, history = train_model(train_df, categorical_cols, numerical_feat_cols, numerical_history_cols, numerical_l_feat_cols, seq_col, target_col,\n","                             vocab_sizes, embedding_dims, device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5471b9b3"},"outputs":[],"source":["plt.figure(figsize=(12,5))\n","plt.subplot(1,2,1)\n","plt.plot(history['train_losses'], label='Train Loss')\n","plt.plot(history['val_losses'], label='Val Loss')\n","plt.legend(); plt.title('Loss'); plt.xlabel('Epoch')\n","plt.subplot(1,2,2)\n","plt.plot(history['train_comp'], label='Train Score')\n","plt.plot(history['val_comp'], label='Val Score')\n","plt.legend(); plt.title('Competition Score'); plt.xlabel('Epoch')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HZwKXT9LHdNd"},"outputs":[],"source":["test_dataset = ClickDataset(test, categorical_cols, numerical_feat_cols, numerical_history_cols, numerical_l_feat_cols, seq_col, has_target=False)\n","test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False, collate_fn=collate_fn_infer, num_workers=2)\n","model.eval()\n","outs = []\n","with torch.no_grad():\n","    for cat_x, num_feat, num_history, num_l_feat, seq_x, seq_len in test_loader:\n","        cat_x, num_feat, num_history, num_l_feat, seq_x, seq_len = cat_x.to(device), num_feat.to(device), num_history.to(device), num_l_feat.to(device), seq_x.to(device), seq_len.to(device)\n","        logits = model(cat_x, num_feat, num_history, num_l_feat, seq_x, seq_len)\n","        outs.append(torch.sigmoid(logits).cpu().numpy())\n","preds = np.concatenate(outs)\n","\n","submission = pd.DataFrame({'ID': test.index, 'clicked': preds})\n","submission.to_csv('./submission.csv', index=False)\n","print(\"Submission saved.\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"1FKYJz73xbG3icNNnD5-7ceKIJUu5HY5e","timestamp":1759737183636}]},"kernelspec":{"display_name":"toss","language":"python","name":"conda"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}