{"cells":[{"cell_type":"markdown","metadata":{"id":"25DgyBWz1uc-"},"source":["# 새로운 시작"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LntquHVd1wN_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759930708927,"user_tz":-540,"elapsed":1255,"user":{"displayName":"김성윤","userId":"09187544673822150877"}},"outputId":"e9374e09-4851-4ddc-e62e-95df1d8234bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n","/gdrive/MyDrive/open\n"]}],"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","\n","%cd /gdrive/MyDrive/open/"]},{"cell_type":"markdown","metadata":{"id":"0c849653"},"source":["## 데이터 로딩 및 전처리\n","\n","### Subtask:\n","이전 셀의 코드를 사용하여 데이터를 로드하고 전처리합니다. 범주형/수치형 컬럼을 사용자 요구사항에 맞게 재정의하고, 각 컬럼 타입에 맞는 스케일링 및 인코딩을 적용합니다. `seq` 컬럼은 문자열 그대로 유지합니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Sl5V-xzECDK"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","\n","all_train = pd.read_parquet(\"./train.parquet\", engine=\"pyarrow\")\n","test = pd.read_parquet(\"./test.parquet\", engine=\"pyarrow\").drop(columns=['ID'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bSJqWXrKmX_o"},"outputs":[],"source":["\n","train_df = all_train.sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","target_col = \"clicked\"\n","seq_col = \"seq\"\n","\n","categorical_cols = ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour']\n","categorical_cols.extend([c for c in train_df.columns if c.startswith('l_feat_') and c not in categorical_cols])\n","\n","numerical_cols = [c for c in train_df.columns if (c.startswith('feat_') or c.startswith('history_')) and c not in categorical_cols and c != seq_col and c != target_col and c != 'ID']\n","\n","all_defined_cols = set(categorical_cols + numerical_cols + [seq_col, target_col, 'ID'])\n","remaining_cols = [c for c in train_df.columns if c not in all_defined_cols]\n","\n","\n","vocab_sizes = {}\n","for col in categorical_cols:\n","    all_categories = pd.concat([all_train[col], test[col]]).astype(str).unique()\n","    vocab_sizes[col] = len(all_categories) + 1\n","    le = LabelEncoder()\n","    le.fit(all_categories)\n","    train_df[col] = le.transform(train_df[col].astype(str))\n","    test[col] = le.transform(test[col].astype(str))\n","    train_df[col] = train_df[col].astype(int)\n","    test[col] = test[col].astype(int)\n","\n","scaler = StandardScaler()\n","train_df[numerical_cols] = scaler.fit_transform(train_df[numerical_cols]).astype(float)\n","test[numerical_cols] = scaler.transform(test[numerical_cols]).astype(float)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nvj5jG49Dh9a"},"outputs":[],"source":["\n","import numpy as np\n","import pandas as pd\n","import torch\n","\n","def debug_dataset(train_df, categorical_cols, vocab_sizes, embedding_dims, transformer_heads):\n","    numeric_df = train_df.select_dtypes(include=[np.number])\n","    nan_counts = numeric_df.isna().sum()\n","    nan_cols = nan_counts[nan_counts > 0]\n","\n","    inf_mask = np.isinf(numeric_df.to_numpy())\n","\n","    for col in categorical_cols:\n","        max_val = train_df[col].max()\n","        min_val = train_df[col].min()\n","        vocab_size = vocab_sizes[col]\n","\n","    total_emb_dim = sum(embedding_dims[col] for col in categorical_cols)\n","    if total_emb_dim % transformer_heads != 0:\n","        pass\n","\n","    if \"seq\" in train_df.columns:\n","        zero_seq_count = (train_df[\"seq\"] == 0).sum()\n","\n","    for col in categorical_cols:\n","        max_idx = train_df[col].max()\n","        min_idx = train_df[col].min()\n","\n","\n","\n","embedding_dims = {}\n","for col in categorical_cols:\n","    if col in ['gender', 'age_group', 'day_of_week', 'hour']:\n","        embedding_dims[col] = 2\n","    elif col == 'inventory_id':\n","        embedding_dims[col] = 4\n","    elif col.startswith('l_feat_'):\n","        vocab_size = vocab_sizes[col]\n","        if vocab_size <= 10:\n","            embedding_dims[col] = 2\n","        elif vocab_size <= 100:\n","            embedding_dims[col] = 4\n","        elif vocab_size <= 1000:\n","            embedding_dims[col] = 8\n","        else:\n","            embedding_dims[col] = 16 # Default for larger vocabularies\n","    else:\n","        embedding_dims[col] = 16 # Default for other categorical columns\n","\n","\n","\n","debug_dataset(\n","    train_df=train_df,\n","    categorical_cols=categorical_cols,\n","    vocab_sizes=vocab_sizes,\n","    embedding_dims=embedding_dims,\n","    transformer_heads=4\n",")\n","\n","import numpy as np\n","numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n","train_df[numeric_cols] = (\n","    train_df[numeric_cols]\n","    .fillna(0)\n","    .replace([np.inf, -np.inf], 0)\n",")"]},{"cell_type":"markdown","metadata":{"id":"ebd87f2d"},"source":["## Dataset 및 DataLoader 수정\n","\n","### Subtask:\n","새로운 모델 구조에 맞게 `ClickDataset` 클래스를 수정하거나 새로 정의합니다. 특히 `seq` 데이터를 시퀀스 형태로 처리하고, 다른 피처들을 분리하여 반환하도록 합니다. Collate 함수도 수정하여 `seq` 데이터 패딩 및 길이를 처리합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"293ad917"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.utils.rnn as rnn_utils\n","import numpy as np\n","import pandas as pd\n","\n","class ClickDataset(Dataset):\n","    def __init__(self, df, categorical_cols, numerical_cols, seq_col, target_col=None, has_target=True):\n","        self.df = df.reset_index(drop=True)\n","        self.categorical_cols = categorical_cols\n","        self.numerical_cols = numerical_cols\n","        self.seq_col = seq_col\n","        self.target_col = target_col\n","        self.has_target = has_target\n","\n","        if self.has_target:\n","            self.y = self.df[self.target_col].astype(np.float32).values\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        cat_features = torch.tensor(self.df.loc[idx, self.categorical_cols].values.astype(int), dtype=torch.long)\n","        num_features = torch.tensor(self.df.loc[idx, self.numerical_cols].values.astype(float), dtype=torch.float)\n","\n","\n","        seq_string = self.df.loc[idx, self.seq_col]\n","        if isinstance(seq_string, str) and seq_string:\n","            try:\n","                seq_array = np.fromstring(seq_string, sep=\",\", dtype=np.float32)\n","            except ValueError:\n","                seq_array = np.array([0.0], dtype=np.float32)\n","        else:\n","            seq_array = np.array([0.0], dtype=np.float32)\n","\n","        seq_tensor = torch.from_numpy(seq_array)\n","\n","        if self.has_target:\n","            y = torch.tensor(self.y[idx], dtype=torch.float)\n","            return cat_features, num_features, seq_tensor, y\n","        else:\n","            return cat_features, num_features, seq_tensor\n","\n","def collate_fn_train(batch):\n","    cat_features, num_features, seqs, ys = zip(*batch)\n","\n","    cat_features = torch.stack(cat_features)\n","    num_features = torch.stack(num_features)\n","    ys = torch.stack(ys).unsqueeze(1)\n","\n","    seqs_padded = rnn_utils.pad_sequence(seqs, batch_first=True, padding_value=0.0)\n","    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n","    seq_lengths = torch.clamp(seq_lengths, min=1)\n","\n","    return cat_features, num_features, seqs_padded, seq_lengths, ys\n","\n","def collate_fn_infer(batch):\n","    cat_features, num_features, seqs = zip(*batch)\n","\n","    cat_features = torch.stack(cat_features)\n","    num_features = torch.stack(num_features)\n","\n","    seqs_padded = rnn_utils.pad_sequence(seqs, batch_first=True, padding_value=0.0)\n","    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n","    seq_lengths = torch.clamp(seq_lengths, min=1)\n","\n","    return cat_features, num_features, seqs_padded, seq_lengths"]},{"cell_type":"markdown","metadata":{"id":"b8996f2c"},"source":["## 모델 구조 정의\n","\n","### Subtask:\n","사용자 요구사항에 맞춰 새로운 `ComplexModel` 클래스를 정의합니다. 이 클래스 안에는 다음 서브 모듈이 포함됩니다.\n","- 범주형 피처를 위한 임베딩 레이어와 Transformer Encoder.\n","- 시퀀스 데이터 처리를 위한 LSTM.\n","- `feat_*_*` 수치형 피처를 위한 MLP.\n","- `history_*_*` 수치형 피처를 위한 MLP.\n","- 모든 서브 모듈의 출력을 결합하고 최종 예측을 수행하는 MLP."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d00a2e59"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","from sklearn.metrics import average_precision_score\n","class ModifiedModel(nn.Module):\n","    def __init__(self, vocab_sizes, embedding_dims, numerical_dim, lstm_hidden_dim=64,\n","                 lstm_n_layers=2, transformer_heads=4, transformer_layers=2, dropout=0.2):\n","        super().__init__()\n","\n","        self.embedding_layers = nn.ModuleDict({col: nn.Embedding(vocab_sizes[col], embedding_dims[col])\n","                                               for col in vocab_sizes.keys()})\n","        total_categorical_embedding_dim = sum(emb.embedding_dim for emb in self.embedding_layers.values())\n","\n","        self.num_norm = nn.BatchNorm1d(numerical_dim)\n","\n","        self.lstm = nn.LSTM(input_size=1, hidden_size=lstm_hidden_dim, num_layers=lstm_n_layers, batch_first=True)\n","\n","        transformer_d_model = total_categorical_embedding_dim + lstm_hidden_dim + numerical_dim\n","\n","        # Ensure transformer_d_model is divisible by transformer_heads\n","        if transformer_d_model % transformer_heads != 0:\n","             # Adjust transformer_d_model to be divisible\n","             print(f\"Warning:{transformer_d_model}%{transformer_heads}!=0\")\n","             transformer_d_model = (transformer_d_model // transformer_heads) * transformer_heads\n","             print(f\"Warning: Adjusted transformer_d_model to {transformer_d_model} to be divisible by {transformer_heads}\")\n","\n","\n","        encoder_layer = TransformerEncoderLayer(d_model=transformer_d_model, nhead=transformer_heads, dropout=dropout, batch_first=True)\n","        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=transformer_layers)\n","\n","        final_mlp_input_dim = transformer_d_model\n","        self.final_mlp = nn.Sequential(\n","            nn.Linear(final_mlp_input_dim, 512),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(512, 256),  # Added hidden layer\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(256, 128), # Added hidden layer\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(128, 1)\n","        )\n","\n","    def forward(self, cat_x, num_x, seq_x, seq_lengths):\n","        embeddings = [self.embedding_layers[col](cat_x[:, i]) for i, col in enumerate(categorical_cols)]\n","        cat_embedded = torch.cat(embeddings, dim=1)\n","        #print(f\"cat_embedded shape: {cat_embedded.shape}\")\n","\n","        num_x = self.num_norm(num_x)\n","        #print(f\"num_x shape: {num_x.shape}\")\n","\n","\n","        seq_x = seq_x.unsqueeze(-1)\n","        packed_seq = nn.utils.rnn.pack_padded_sequence(seq_x, seq_lengths.cpu(), batch_first=True, enforce_sorted=False)\n","        _, (h_n, _) = self.lstm(packed_seq)\n","        lstm_output = h_n[-1]\n","        #print(f\"lstm_output shape: {lstm_output.shape}\")\n","\n","\n","        combined_features = torch.cat([cat_embedded, num_x, lstm_output], dim=1)\n","        #print(f\"combined_features shape before adjustment: {combined_features.shape}\")\n","\n","\n","        # Pad or truncate combined_features to match the adjusted transformer_d_model\n","        target_dim = self.transformer_encoder.layers[0].self_attn.embed_dim\n","        if combined_features.size(1) != target_dim:\n","             # Create a new tensor with the correct size and copy the data\n","             adjusted_combined_features = torch.zeros(combined_features.size(0), target_dim, device=combined_features.device)\n","             copy_size = min(combined_features.size(1), adjusted_combined_features.size(1))\n","             adjusted_combined_features[:, :copy_size] = combined_features[:, :copy_size]\n","             combined_features = adjusted_combined_features\n","             #print(f\"combined_features shape after adjustment: {combined_features.shape}\")\n","\n","\n","        transformer_output = self.transformer_encoder(combined_features.unsqueeze(1)).squeeze(1)\n","        #print(f\"transformer_output shape: {transformer_output.shape}\")\n","\n","\n","        logits = self.final_mlp(transformer_output).squeeze(1)\n","        return logits\n","\n","\n","\n","\n","def calculate_weighted_logloss(y_true, y_pred, eps=1e-15):\n","    y_pred = np.clip(y_pred, eps, 1 - eps)\n","    mask_0 = (y_true == 0)\n","    mask_1 = (y_true == 1)\n","    ll_0 = -np.mean(np.log(1 - y_pred[mask_0])) if mask_0.sum() > 0 else 0\n","    ll_1 = -np.mean(np.log(y_pred[mask_1])) if mask_1.sum() > 0 else 0\n","    return 0.5 * ll_0 + 0.5 * ll_1\n","\n","def calculate_competition_score(y_true, y_pred):\n","    ap = average_precision_score(y_true, y_pred)\n","    wll = calculate_weighted_logloss(y_true, y_pred)\n","    score = 0.5 * ap + 0.5 * (1 / (1 + wll))\n","    return score, ap, wll"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a23ec418"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, random_split\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","import os\n","from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR, CosineAnnealingWarmRestarts # Import the new scheduler\n","\n","def train_complex_model(train_df, categorical_cols, numerical_cols, seq_col, target_col,\n","                         vocab_sizes, embedding_dims, lstm_hidden_dims=64, lstm_layers=2, transformer_n_heads=4, transformer_n_layers=2,\n","                         n_dropout=0.2, batch_size=512, epochs=3, lr=1e-3, device=\"cuda\"):\n","\n","    tr_df, va_df = train_test_split(train_df, test_size=0.2, random_state=42, shuffle=True)\n","\n","    train_dataset = ClickDataset(tr_df, categorical_cols, numerical_cols, seq_col, target_col, has_target=True)\n","    val_dataset = ClickDataset(va_df, categorical_cols, numerical_cols, seq_col, target_col, has_target=True)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_train, num_workers=2)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_train, num_workers=2)\n","\n","    numerical_dim = len(numerical_cols)\n","\n","    model = ModifiedModel(\n","        vocab_sizes=vocab_sizes,\n","        embedding_dims=embedding_dims,\n","        numerical_dim=numerical_dim,\n","        lstm_hidden_dim=lstm_hidden_dims,\n","        lstm_n_layers=lstm_layers,\n","        transformer_heads=transformer_n_heads,\n","        transformer_layers=transformer_n_layers,\n","        dropout=n_dropout\n","    ).to(device)\n","\n","    pos_count = tr_df[target_col].sum()\n","    neg_count = len(tr_df) - pos_count\n","    pos_weight = torch.tensor(neg_count / pos_count, dtype=torch.float32).to(device)\n","\n","    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n","\n","    # Replace CosineAnnealingLR with CosineAnnealingWarmRestarts\n","    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=2, T_mult=2)\n","\n","\n","    train_losses = []\n","    val_losses = []\n","    train_ap_scores = []\n","    val_ap_scores = []\n","    train_wll_scores = []\n","    val_wll_scores = []\n","    train_comp_scores = []\n","    val_comp_scores = []\n","\n","    best_val_comp_score = -float('inf')\n","    best_model_state_dict = None\n","    model_save_path = './T2G_simple_best_model.pth'\n","\n","    for epoch in range(1, epochs + 1):\n","        model.train()\n","        train_loss = 0.0\n","        train_preds = []\n","        train_true = []\n","        for cat_features, num_features, seqs_padded, seq_lengths, ys in tqdm(train_loader, desc=f\"Train Epoch {epoch}\"):\n","            cat_features, num_features, seqs_padded, seq_lengths, ys = cat_features.to(device), num_features.to(device), seqs_padded.to(device), seq_lengths.to(device), ys.to(device)\n","\n","            optimizer.zero_grad()\n","            logits = model(cat_features, num_features, seqs_padded, seq_lengths)\n","            loss = criterion(logits, ys.squeeze(1))\n","\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item() * ys.size(0)\n","\n","            train_preds.append(torch.sigmoid(logits).detach().cpu().numpy())\n","            train_true.append(ys.squeeze(1).cpu().numpy())\n","\n","        train_loss /= len(train_dataset)\n","        train_preds = np.concatenate(train_preds)\n","        train_true = np.concatenate(train_true)\n","        train_comp_score, train_ap, train_wll = calculate_competition_score(train_true, train_preds)\n","\n","        train_losses.append(train_loss)\n","        train_ap_scores.append(train_ap)\n","        train_wll_scores.append(train_wll)\n","        train_comp_scores.append(train_comp_score)\n","\n","        model.eval()\n","        val_loss = 0.0\n","        val_preds = []\n","        val_true = []\n","        with torch.no_grad():\n","            for cat_features, num_features, seqs_padded, seq_lengths, ys in tqdm(val_loader, desc=f\"Val Epoch {epoch}\"):\n","                 cat_features, num_features, seqs_padded, seq_lengths, ys = cat_features.to(device), num_features.to(device), seqs_padded.to(device), seq_lengths.to(device), ys.to(device)\n","\n","                 logits = model(cat_features, num_features, seqs_padded, seq_lengths)\n","                 loss = criterion(logits, ys.squeeze(1))\n","                 val_loss += loss.item() * len(ys)\n","\n","                 val_preds.append(torch.sigmoid(logits).cpu().numpy())\n","                 val_true.append(ys.squeeze(1).cpu().numpy())\n","\n","        val_loss /= len(val_dataset)\n","        val_preds = np.concatenate(val_preds)\n","        val_true = np.concatenate(val_true)\n","        val_comp_score, val_ap, val_wll = calculate_competition_score(val_true, val_preds)\n","\n","        val_losses.append(val_loss)\n","        val_ap_scores.append(val_ap)\n","        val_wll_scores.append(val_wll)\n","        val_comp_scores.append(val_comp_score)\n","\n","        scheduler.step() # Scheduler step is typically called after each epoch\n","\n","        if val_comp_score > best_val_comp_score:\n","            best_val_comp_score = val_comp_score\n","            best_model_state_dict = model.state_dict()\n","            torch.save(best_model_state_dict, model_save_path)\n","\n","        print(f\"[Epoch {epoch}] Train Loss: {train_loss:.4f} | Train Comp Score: {train_comp_score:.4f} (AP: {train_ap:.4f}, WLL: {train_wll:.4f}) | Val Loss: {val_loss:.4f} | Val Comp Score: {val_comp_score:.4f} (AP: {val_ap:.4f}, WLL: {val_wll:.4f})\")\n","\n","    if best_model_state_dict:\n","        model.load_state_dict(best_model_state_dict)\n","\n","    return model, {\n","        'train_losses': train_losses,\n","        'val_losses': val_losses,\n","        'train_ap_scores': train_ap_scores,\n","        'val_ap_scores': val_ap_scores,\n","        'train_wll_scores': train_wll_scores,\n","        'val_wll_scores': val_wll_scores,\n","        'train_comp_scores': train_comp_scores,\n","        'val_comp_scores': val_comp_scores\n","    }"]},{"cell_type":"markdown","metadata":{"id":"db01c42e"},"source":["## 모델 학습, 추론 및 제출 파일 생성 (컬럼 목록 정의 포함)\n","\n","### Subtask:\n","정의된 `ComplexModel`을 사용하여 모델을 학습하고, 학습된 모델로 테스트 데이터에 대한 추론을 수행하여 제출 파일을 생성합니다. 필요한 컬럼 목록 변수들을 이 셀 내에서 명확히 정의합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tsCGTsXy7vKF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759931376982,"user_tz":-540,"elapsed":22,"user":{"displayName":"김성윤","userId":"09187544673822150877"}},"outputId":"3c4320e2-beb3-4bd2-f5bd-d518d2026621"},"outputs":[{"output_type":"stream","name":"stdout","text":["device : cuda\n"]}],"source":["CFG = {\n","    'BATCH_SIZE': 1024,\n","    'EPOCHS': 5,\n","    'LEARNING_RATE': 1e-2,\n","    'SEED' : 42,\n","    'lstm_hidden_dims' : 32,\n","    'lstm_layers':2,\n","    'transformer_n_heads':4,\n","    'transformer_n_layers':2,\n","    'n_dropout' : 0.2\n","}\n","\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f'device : {device}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":428},"executionInfo":{"elapsed":702173,"status":"error","timestamp":1759932079157,"user":{"displayName":"김성윤","userId":"09187544673822150877"},"user_tz":-540},"id":"45a6773c","outputId":"3b1838cf-9f79-4b34-b2ab-00d2f76b317d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Warning:301%4!=0\n","Warning: Adjusted transformer_d_model to 300 to be divisible by 4\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 1:  13%|█▎        | 1081/8363 [10:59<1:14:03,  1.64it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1739863041.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model, history = train_complex_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mtrain_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mcategorical_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mnumerical_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumerical_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mseq_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1379820248.py\u001b[0m in \u001b[0;36mtrain_complex_model\u001b[0;34m(train_df, categorical_cols, numerical_cols, seq_col, target_col, vocab_sizes, embedding_dims, lstm_hidden_dims, lstm_layers, transformer_n_heads, transformer_n_layers, n_dropout, batch_size, epochs, lr, device)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mtrain_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mtrain_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseqs_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Train Epoch {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mcat_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseqs_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseqs_padded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1455\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1136\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["model, history = train_complex_model(\n","            train_df=train_df,\n","            categorical_cols=categorical_cols,\n","            numerical_cols=numerical_cols,\n","            seq_col=seq_col,\n","            target_col=target_col,\n","            vocab_sizes=vocab_sizes,\n","            embedding_dims=embedding_dims,\n","            lstm_hidden_dims = CFG['lstm_hidden_dims'],\n","            lstm_layers = CFG['lstm_layers'],\n","            transformer_n_heads=CFG['transformer_n_heads'],\n","            transformer_n_layers=CFG['transformer_n_layers'],\n","            n_dropout=CFG['n_dropout'],\n","            batch_size=CFG['BATCH_SIZE'],\n","            epochs=CFG['EPOCHS'],\n","            lr=CFG['LEARNING_RATE'],\n","            device=device\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5471b9b3"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(12, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(history['train_losses'], label='Train Loss')\n","plt.plot(history['val_losses'], label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(history['train_comp_scores'], label='Train Comp Score')\n","plt.plot(history['val_comp_scores'], label='Validation Comp Score')\n","plt.xlabel('Epoch')\n","plt.ylabel('Competition Score')\n","plt.title('Training and Validation Competition Score')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()\n","\n","plt.figure(figsize=(12, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(history['train_ap_scores'], label='Train AP')\n","plt.plot(history['val_ap_scores'], label='Validation AP')\n","plt.xlabel('Epoch')\n","plt.ylabel('AP Score')\n","plt.title('Training and Validation AP Score')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(history['train_wll_scores'], label='Train WLL')\n","plt.plot(history['val_wll_scores'], label='Validation WLL')\n","plt.xlabel('Epoch')\n","plt.ylabel('WLL Score')\n","plt.title('Training and Validation WLL Score')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b66d4f53"},"outputs":[],"source":["import numpy as np\n","\n","initial_missing_values = test.isnull().sum().sum()\n","if initial_missing_values > 0:\n","    print(f\"Initial missing values in test set: {initial_missing_values}\")\n","    test.fillna(0, inplace=True)\n","    remaining_missing_values = test.isnull().sum().sum()\n","    print(f\"Remaining missing values in test set after filling with 0: {remaining_missing_values}\")\n","\n","\n","test_numeric_cols = test.select_dtypes(include=[np.number])\n","test_inf_values = np.isinf(test_numeric_cols.to_numpy()).sum()\n","if test_inf_values > 0:\n","    print(f\"Infinite values found in test set: {test_inf_values}\")\n","    inf_cols_test = test_numeric_cols.columns[np.any(np.isinf(test_numeric_cols.to_numpy()), axis=0)]\n","    print(f\"Columns with infinite values in test set: {inf_cols_test.tolist()}\")\n","    # Handle infinite values if necessary, e.g., replace with NaN or a large number\n","    # test.replace([np.inf, -np.inf], np.nan, inplace=True) # Example replacement\n","\n","empty_seq_count = test['seq'].astype(str).apply(lambda x: not bool(x.strip())).sum()\n","if empty_seq_count > 0:\n","    print(f\"Empty 'seq' strings found in test set: {empty_seq_count}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8gYuRaqggEk4"},"outputs":[],"source":["# model = ModifiedModel(\n","#         vocab_sizes=vocab_sizes,\n","#         embedding_dims=embedding_dims,\n","#         numerical_dim=len(numerical_cols),\n","#         lstm_hidden_dims = CFG['lstm_hidden_dims'],\n","#         lstm_layers = CFG['lstm_layers'],\n","#         transformer_n_heads=CFG['transformer_n_heads'],\n","#         transformer_n_layers=CFG['transformer_n_layers'],\n","#         n_dropout=CFG['n_dropout'],\n","#     ).to(device)\n","\n","# model.load_state_dict(torch.load('./T2G_simple_best_model.pth'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HZwKXT9LHdNd"},"outputs":[],"source":["import torch\n","import pandas as pd\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","\n","test_dataset = ClickDataset(test, categorical_cols, numerical_cols, seq_col, has_target=False)\n","test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, collate_fn=collate_fn_infer, num_workers=2)\n","\n","model.eval()\n","outs = []\n","with torch.no_grad():\n","    for cat_features, num_features, seqs_padded, seq_lengths in tqdm(test_loader, desc=\"Inference\"):\n","        cat_features, num_features, seqs_padded, seq_lengths = cat_features.to(device), num_features.to(device), seqs_padded.to(device), seq_lengths.to(device)\n","        logits = model(cat_features, num_features, seqs_padded, seq_lengths)\n","        predictions = torch.sigmoid(logits).cpu()\n","        outs.append(predictions)\n","\n","test_preds = torch.cat(outs).numpy()\n","\n","submit = pd.read_csv('./sample_submission.csv')\n","submit['clicked'] = test_preds\n","submit.to_csv('./T2G_light_submission.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ezx6t3nxKWjB"},"outputs":[],"source":["import pandas as pd\n","submit_df = pd.read_csv('./T2G_simple_submission.csv')\n","missing_values_count = submit_df['clicked'].isnull().sum()\n","total_missing_all_cols = submit_df.isnull().sum().sum()\n","print(f\"Missing values in 'clicked' column: {missing_values_count}\")\n","print(f\"Total missing values in all columns: {total_missing_all_cols}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMBTMs50FuLL"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A0XDhRneRboc"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[{"file_id":"1xPv1vET0NoeI_N-Xi7RsODThBs6ZV67W","timestamp":1759846108866},{"file_id":"1FKYJz73xbG3icNNnD5-7ceKIJUu5HY5e","timestamp":1759737183636}]},"kernelspec":{"display_name":"toss","language":"python","name":"conda"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}